{
  "repo": "scikit-learn/scikit-learn",
  "instance_id": "scikit-learn__scikit-learn-13174",
  "patch": "diff --git a/doc/whats_new/v0.21.rst b/doc/whats_new/v0.21.rst\nindex 0b7adfbd60376..43703cde1ddc1 100644\n--- a/doc/whats_new/v0.21.rst\n+++ b/doc/whats_new/v0.21.rst\n@@ -118,6 +118,10 @@ Support for Python 3.4 and below has been officially dropped.\n   value of ``learning_rate`` in ``update_terminal_regions`` is not consistent\n   with the document and the caller functions.\n   :issue:`6463` by :user:`movelikeriver <movelikeriver>`.\n+  \n+- |Enhancement| Minimized the validation of X in\n+  :class:`ensemble.AdaBoostClassifier` and :class:`ensemble.AdaBoostRegressor`\n+  :issue:`13174` by :user:`Christos Aridas <chkoar>`.\n \n :mod:`sklearn.externals`\n ........................\ndiff --git a/sklearn/ensemble/weight_boosting.py b/sklearn/ensemble/weight_boosting.py\nindex f82a0d882b873..1a8fb7e1243d7 100644\n--- a/sklearn/ensemble/weight_boosting.py\n+++ b/sklearn/ensemble/weight_boosting.py\n@@ -30,16 +30,15 @@\n from scipy.special import xlogy\n \n from .base import BaseEnsemble\n-from ..base import ClassifierMixin, RegressorMixin, is_regressor, is_classifier\n+from ..base import ClassifierMixin, RegressorMixin, is_classifier, is_regressor\n \n-from .forest import BaseForest\n from ..tree import DecisionTreeClassifier, DecisionTreeRegressor\n-from ..tree.tree import BaseDecisionTree\n-from ..tree._tree import DTYPE\n-from ..utils import check_array, check_X_y, check_random_state\n+from ..utils import check_array, check_random_state, check_X_y, safe_indexing\n from ..utils.extmath import stable_cumsum\n from ..metrics import accuracy_score, r2_score\n-from sklearn.utils.validation import has_fit_parameter, check_is_fitted\n+from ..utils.validation import check_is_fitted\n+from ..utils.validation import has_fit_parameter\n+from ..utils.validation import _num_samples\n \n __all__ = [\n     'AdaBoostClassifier',\n@@ -70,6 +69,26 @@ def __init__(self,\n         self.learning_rate = learning_rate\n         self.random_state = random_state\n \n+    def _validate_data(self, X, y=None):\n+\n+        # Accept or convert to these sparse matrix formats so we can\n+        # use safe_indexing\n+        accept_sparse = ['csr', 'csc']\n+        if y is None:\n+            ret = check_array(X,\n+                              accept_sparse=accept_sparse,\n+                              ensure_2d=False,\n+                              allow_nd=True,\n+                              dtype=None)\n+        else:\n+            ret = check_X_y(X, y,\n+                            accept_sparse=accept_sparse,\n+                            ensure_2d=False,\n+                            allow_nd=True,\n+                            dtype=None,\n+                            y_numeric=is_regressor(self))\n+        return ret\n+\n     def fit(self, X, y, sample_weight=None):\n         \"\"\"Build a boosted classifier/regressor from the training set (X, y).\n \n@@ -77,9 +96,7 @@ def fit(self, X, y, sample_weight=None):\n         ----------\n         X : {array-like, sparse matrix} of shape = [n_samples, n_features]\n             The training input samples. Sparse matrix can be CSC, CSR, COO,\n-            DOK, or LIL. COO, DOK, and LIL are converted to CSR. The dtype is\n-            forced to DTYPE from tree._tree if the base classifier of this\n-            ensemble weighted boosting classifier is a tree or forest.\n+            DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n \n         y : array-like of shape = [n_samples]\n             The target values (class labels in classification, real numbers in\n@@ -97,22 +114,12 @@ def fit(self, X, y, sample_weight=None):\n         if self.learning_rate <= 0:\n             raise ValueError(\"learning_rate must be greater than zero\")\n \n-        if (self.base_estimator is None or\n-                isinstance(self.base_estimator, (BaseDecisionTree,\n-                                                 BaseForest))):\n-            dtype = DTYPE\n-            accept_sparse = 'csc'\n-        else:\n-            dtype = None\n-            accept_sparse = ['csr', 'csc']\n-\n-        X, y = check_X_y(X, y, accept_sparse=accept_sparse, dtype=dtype,\n-                         y_numeric=is_regressor(self))\n+        X, y = self._validate_data(X, y)\n \n         if sample_weight is None:\n             # Initialize weights to 1 / n_samples\n-            sample_weight = np.empty(X.shape[0], dtype=np.float64)\n-            sample_weight[:] = 1. / X.shape[0]\n+            sample_weight = np.empty(_num_samples(X), dtype=np.float64)\n+            sample_weight[:] = 1. / _num_samples(X)\n         else:\n             sample_weight = check_array(sample_weight, ensure_2d=False)\n             # Normalize existing weights\n@@ -216,7 +223,7 @@ def staged_score(self, X, y, sample_weight=None):\n         ----------\n         X : {array-like, sparse matrix} of shape = [n_samples, n_features]\n             The training input samples. Sparse matrix can be CSC, CSR, COO,\n-            DOK, or LIL. DOK and LIL are converted to CSR.\n+            DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n \n         y : array-like, shape = [n_samples]\n             Labels for X.\n@@ -228,6 +235,8 @@ def staged_score(self, X, y, sample_weight=None):\n         -------\n         z : float\n         \"\"\"\n+        X = self._validate_data(X)\n+\n         for y_pred in self.staged_predict(X):\n             if is_classifier(self):\n                 yield accuracy_score(y, y_pred, sample_weight=sample_weight)\n@@ -259,18 +268,6 @@ def feature_importances_(self):\n                 \"since base_estimator does not have a \"\n                 \"feature_importances_ attribute\")\n \n-    def _validate_X_predict(self, X):\n-        \"\"\"Ensure that X is in the proper format\"\"\"\n-        if (self.base_estimator is None or\n-                isinstance(self.base_estimator,\n-                           (BaseDecisionTree, BaseForest))):\n-            X = check_array(X, accept_sparse='csr', dtype=DTYPE)\n-\n-        else:\n-            X = check_array(X, accept_sparse=['csr', 'csc', 'coo'])\n-\n-        return X\n-\n \n def _samme_proba(estimator, n_classes, X):\n     \"\"\"Calculate algorithm 4, step 2, equation c) of Zhu et al [1].\n@@ -391,7 +388,7 @@ def fit(self, X, y, sample_weight=None):\n         ----------\n         X : {array-like, sparse matrix} of shape = [n_samples, n_features]\n             The training input samples. Sparse matrix can be CSC, CSR, COO,\n-            DOK, or LIL. DOK and LIL are converted to CSR.\n+            DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n \n         y : array-like of shape = [n_samples]\n             The target values (class labels).\n@@ -442,8 +439,7 @@ def _boost(self, iboost, X, y, sample_weight, random_state):\n             The index of the current boost iteration.\n \n         X : {array-like, sparse matrix} of shape = [n_samples, n_features]\n-            The training input samples. Sparse matrix can be CSC, CSR, COO,\n-            DOK, or LIL. DOK and LIL are converted to CSR.\n+            The training input samples.\n \n         y : array-like of shape = [n_samples]\n             The target values (class labels).\n@@ -591,13 +587,15 @@ def predict(self, X):\n         ----------\n         X : {array-like, sparse matrix} of shape = [n_samples, n_features]\n             The training input samples. Sparse matrix can be CSC, CSR, COO,\n-            DOK, or LIL. DOK and LIL are converted to CSR.\n+            DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n \n         Returns\n         -------\n         y : array of shape = [n_samples]\n             The predicted classes.\n         \"\"\"\n+        X = self._validate_data(X)\n+\n         pred = self.decision_function(X)\n \n         if self.n_classes_ == 2:\n@@ -618,13 +616,16 @@ def staged_predict(self, X):\n         Parameters\n         ----------\n         X : array-like of shape = [n_samples, n_features]\n-            The input samples.\n+            The input samples. Sparse matrix can be CSC, CSR, COO,\n+            DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n \n         Returns\n         -------\n         y : generator of array, shape = [n_samples]\n             The predicted classes.\n         \"\"\"\n+        X = self._validate_data(X)\n+\n         n_classes = self.n_classes_\n         classes = self.classes_\n \n@@ -644,7 +645,7 @@ def decision_function(self, X):\n         ----------\n         X : {array-like, sparse matrix} of shape = [n_samples, n_features]\n             The training input samples. Sparse matrix can be CSC, CSR, COO,\n-            DOK, or LIL. DOK and LIL are converted to CSR.\n+            DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n \n         Returns\n         -------\n@@ -657,7 +658,7 @@ def decision_function(self, X):\n             class in ``classes_``, respectively.\n         \"\"\"\n         check_is_fitted(self, \"n_classes_\")\n-        X = self._validate_X_predict(X)\n+        X = self._validate_data(X)\n \n         n_classes = self.n_classes_\n         classes = self.classes_[:, np.newaxis]\n@@ -687,7 +688,7 @@ def staged_decision_function(self, X):\n         ----------\n         X : {array-like, sparse matrix} of shape = [n_samples, n_features]\n             The training input samples. Sparse matrix can be CSC, CSR, COO,\n-            DOK, or LIL. DOK and LIL are converted to CSR.\n+            DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n \n         Returns\n         -------\n@@ -700,7 +701,7 @@ def staged_decision_function(self, X):\n             class in ``classes_``, respectively.\n         \"\"\"\n         check_is_fitted(self, \"n_classes_\")\n-        X = self._validate_X_predict(X)\n+        X = self._validate_data(X)\n \n         n_classes = self.n_classes_\n         classes = self.classes_[:, np.newaxis]\n@@ -741,7 +742,7 @@ def predict_proba(self, X):\n         ----------\n         X : {array-like, sparse matrix} of shape = [n_samples, n_features]\n             The training input samples. Sparse matrix can be CSC, CSR, COO,\n-            DOK, or LIL. DOK and LIL are converted to CSR.\n+            DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n \n         Returns\n         -------\n@@ -750,12 +751,12 @@ def predict_proba(self, X):\n             outputs is the same of that of the `classes_` attribute.\n         \"\"\"\n         check_is_fitted(self, \"n_classes_\")\n+        X = self._validate_data(X)\n \n         n_classes = self.n_classes_\n-        X = self._validate_X_predict(X)\n \n         if n_classes == 1:\n-            return np.ones((X.shape[0], 1))\n+            return np.ones((_num_samples(X), 1))\n \n         if self.algorithm == 'SAMME.R':\n             # The weights are all 1. for SAMME.R\n@@ -790,7 +791,7 @@ def staged_predict_proba(self, X):\n         ----------\n         X : {array-like, sparse matrix} of shape = [n_samples, n_features]\n             The training input samples. Sparse matrix can be CSC, CSR, COO,\n-            DOK, or LIL. DOK and LIL are converted to CSR.\n+            DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n \n         Returns\n         -------\n@@ -798,7 +799,7 @@ def staged_predict_proba(self, X):\n             The class probabilities of the input samples. The order of\n             outputs is the same of that of the `classes_` attribute.\n         \"\"\"\n-        X = self._validate_X_predict(X)\n+        X = self._validate_data(X)\n \n         n_classes = self.n_classes_\n         proba = None\n@@ -837,7 +838,7 @@ def predict_log_proba(self, X):\n         ----------\n         X : {array-like, sparse matrix} of shape = [n_samples, n_features]\n             The training input samples. Sparse matrix can be CSC, CSR, COO,\n-            DOK, or LIL. DOK and LIL are converted to CSR.\n+            DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n \n         Returns\n         -------\n@@ -845,6 +846,7 @@ def predict_log_proba(self, X):\n             The class probabilities of the input samples. The order of\n             outputs is the same of that of the `classes_` attribute.\n         \"\"\"\n+        X = self._validate_data(X)\n         return np.log(self.predict_proba(X))\n \n \n@@ -937,7 +939,7 @@ def fit(self, X, y, sample_weight=None):\n         ----------\n         X : {array-like, sparse matrix} of shape = [n_samples, n_features]\n             The training input samples. Sparse matrix can be CSC, CSR, COO,\n-            DOK, or LIL. DOK and LIL are converted to CSR.\n+            DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n \n         y : array-like of shape = [n_samples]\n             The target values (real numbers).\n@@ -975,8 +977,7 @@ def _boost(self, iboost, X, y, sample_weight, random_state):\n             The index of the current boost iteration.\n \n         X : {array-like, sparse matrix} of shape = [n_samples, n_features]\n-            The training input samples. Sparse matrix can be CSC, CSR, COO,\n-            DOK, or LIL. DOK and LIL are converted to CSR.\n+            The training input samples.\n \n         y : array-like of shape = [n_samples]\n             The target values (class labels in classification, real numbers in\n@@ -1008,14 +1009,16 @@ def _boost(self, iboost, X, y, sample_weight, random_state):\n         # For NumPy >= 1.7.0 use np.random.choice\n         cdf = stable_cumsum(sample_weight)\n         cdf /= cdf[-1]\n-        uniform_samples = random_state.random_sample(X.shape[0])\n+        uniform_samples = random_state.random_sample(_num_samples(X))\n         bootstrap_idx = cdf.searchsorted(uniform_samples, side='right')\n         # searchsorted returns a scalar\n         bootstrap_idx = np.array(bootstrap_idx, copy=False)\n \n         # Fit on the bootstrapped sample and obtain a prediction\n         # for all samples in the training set\n-        estimator.fit(X[bootstrap_idx], y[bootstrap_idx])\n+        X_ = safe_indexing(X, bootstrap_idx)\n+        y_ = safe_indexing(y, bootstrap_idx)\n+        estimator.fit(X_, y_)\n         y_predict = estimator.predict(X)\n \n         error_vect = np.abs(y_predict - y)\n@@ -1067,10 +1070,10 @@ def _get_median_predict(self, X, limit):\n         median_or_above = weight_cdf >= 0.5 * weight_cdf[:, -1][:, np.newaxis]\n         median_idx = median_or_above.argmax(axis=1)\n \n-        median_estimators = sorted_idx[np.arange(X.shape[0]), median_idx]\n+        median_estimators = sorted_idx[np.arange(_num_samples(X)), median_idx]\n \n         # Return median predictions\n-        return predictions[np.arange(X.shape[0]), median_estimators]\n+        return predictions[np.arange(_num_samples(X)), median_estimators]\n \n     def predict(self, X):\n         \"\"\"Predict regression value for X.\n@@ -1082,7 +1085,7 @@ def predict(self, X):\n         ----------\n         X : {array-like, sparse matrix} of shape = [n_samples, n_features]\n             The training input samples. Sparse matrix can be CSC, CSR, COO,\n-            DOK, or LIL. DOK and LIL are converted to CSR.\n+            DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n \n         Returns\n         -------\n@@ -1090,7 +1093,7 @@ def predict(self, X):\n             The predicted regression values.\n         \"\"\"\n         check_is_fitted(self, \"estimator_weights_\")\n-        X = self._validate_X_predict(X)\n+        X = self._validate_data(X)\n \n         return self._get_median_predict(X, len(self.estimators_))\n \n@@ -1107,8 +1110,7 @@ def staged_predict(self, X):\n         Parameters\n         ----------\n         X : {array-like, sparse matrix} of shape = [n_samples, n_features]\n-            The training input samples. Sparse matrix can be CSC, CSR, COO,\n-            DOK, or LIL. DOK and LIL are converted to CSR.\n+            The training input samples.\n \n         Returns\n         -------\n@@ -1116,7 +1118,7 @@ def staged_predict(self, X):\n             The predicted regression values.\n         \"\"\"\n         check_is_fitted(self, \"estimator_weights_\")\n-        X = self._validate_X_predict(X)\n+        X = self._validate_data(X)\n \n         for i, _ in enumerate(self.estimators_, 1):\n             yield self._get_median_predict(X, limit=i)\n",
  "test_patch": "diff --git a/sklearn/ensemble/tests/test_weight_boosting.py b/sklearn/ensemble/tests/test_weight_boosting.py\nindex 435d2ef30e97c..adeaf1d824fe8 100755\n--- a/sklearn/ensemble/tests/test_weight_boosting.py\n+++ b/sklearn/ensemble/tests/test_weight_boosting.py\n@@ -471,7 +471,6 @@ def fit(self, X, y, sample_weight=None):\n def test_sample_weight_adaboost_regressor():\n     \"\"\"\n     AdaBoostRegressor should work without sample_weights in the base estimator\n-\n     The random weighted sampling is done internally in the _boost method in\n     AdaBoostRegressor.\n     \"\"\"\n@@ -486,3 +485,27 @@ def predict(self, X):\n     boost = AdaBoostRegressor(DummyEstimator(), n_estimators=3)\n     boost.fit(X, y_regr)\n     assert_equal(len(boost.estimator_weights_), len(boost.estimator_errors_))\n+\n+\n+def test_multidimensional_X():\n+    \"\"\"\n+    Check that the AdaBoost estimators can work with n-dimensional\n+    data matrix\n+    \"\"\"\n+\n+    from sklearn.dummy import DummyClassifier, DummyRegressor\n+\n+    rng = np.random.RandomState(0)\n+\n+    X = rng.randn(50, 3, 3)\n+    yc = rng.choice([0, 1], 50)\n+    yr = rng.randn(50)\n+\n+    boost = AdaBoostClassifier(DummyClassifier(strategy='most_frequent'))\n+    boost.fit(X, yc)\n+    boost.predict(X)\n+    boost.predict_proba(X)\n+\n+    boost = AdaBoostRegressor(DummyRegressor())\n+    boost.fit(X, yr)\n+    boost.predict(X)\n",
  "base_commit": "09bc27630fb8feea2f10627dce25e93cd6ff258a",
  "head_commit": "1a2e896209aa85e8480f03800cb2ce0375283987",
  "created_at": 1550270263000,
  "version": "0.21",
  "duration_changes": "[{\"sklearn/tests/test_common.py::test_non_meta_estimators[MiniBatchKMeans-MiniBatchKMeans-check_sample_weights_pandas_series]\": {\"base\": 0.23747438192367554, \"head\": 0.24469535425305367}, \"sklearn/tests/test_common.py::test_non_meta_estimators[NMF-NMF-check_fit_idempotent]\": {\"base\": 0.017660804092884064, \"head\": 0.055859632790088654}, \"sklearn/tests/test_common.py::test_non_meta_estimators[QuantileTransformer-QuantileTransformer-check_dtype_object]\": {\"base\": 0.04766733571887016, \"head\": 0.039875201880931854}}, {\"sklearn/tests/test_common.py::test_non_meta_estimators[MiniBatchKMeans-MiniBatchKMeans-check_sample_weights_pandas_series]\": {\"base\": 0.2345668375492096, \"head\": 0.22758888080716133}, \"sklearn/tests/test_common.py::test_non_meta_estimators[NMF-NMF-check_fit_idempotent]\": {\"base\": 0.027891356498003006, \"head\": 0.036520641297101974}, \"sklearn/tests/test_common.py::test_non_meta_estimators[QuantileTransformer-QuantileTransformer-check_dtype_object]\": {\"base\": 0.05971875786781311, \"head\": 0.09956064075231552}}, {\"sklearn/tests/test_common.py::test_non_meta_estimators[MiniBatchKMeans-MiniBatchKMeans-check_sample_weights_pandas_series]\": {\"base\": 0.23258670046925545, \"head\": 0.2291823886334896}, \"sklearn/tests/test_common.py::test_non_meta_estimators[NMF-NMF-check_fit_idempotent]\": {\"base\": 0.10582603886723518, \"head\": 0.05123378336429596}, \"sklearn/tests/test_common.py::test_non_meta_estimators[QuantileTransformer-QuantileTransformer-check_dtype_object]\": {\"base\": 0.0893731489777565, \"head\": 0.02017896994948387}}, {\"sklearn/tests/test_common.py::test_non_meta_estimators[MiniBatchKMeans-MiniBatchKMeans-check_sample_weights_pandas_series]\": {\"base\": 0.266366820782423, \"head\": 0.24955150112509727}, \"sklearn/tests/test_common.py::test_non_meta_estimators[NMF-NMF-check_fit_idempotent]\": {\"base\": 0.053836576640605927, \"head\": 0.04016517475247383}, \"sklearn/tests/test_common.py::test_non_meta_estimators[QuantileTransformer-QuantileTransformer-check_dtype_object]\": {\"base\": 0.09253070503473282, \"head\": 0.10270850360393524}}, {\"sklearn/tests/test_common.py::test_non_meta_estimators[MiniBatchKMeans-MiniBatchKMeans-check_sample_weights_pandas_series]\": {\"base\": 0.2821670025587082, \"head\": 0.23039580881595612}, \"sklearn/tests/test_common.py::test_non_meta_estimators[NMF-NMF-check_fit_idempotent]\": {\"base\": 0.02495882660150528, \"head\": 0.019018903374671936}, \"sklearn/tests/test_common.py::test_non_meta_estimators[QuantileTransformer-QuantileTransformer-check_dtype_object]\": {\"base\": 0.04042430222034454, \"head\": 0.016721397638320923}}, {\"sklearn/tests/test_common.py::test_non_meta_estimators[MiniBatchKMeans-MiniBatchKMeans-check_sample_weights_pandas_series]\": {\"base\": 0.2438671737909317, \"head\": 0.23045943677425385}, \"sklearn/tests/test_common.py::test_non_meta_estimators[NMF-NMF-check_fit_idempotent]\": {\"base\": 0.07368536666035652, \"head\": 0.01746588572859764}, \"sklearn/tests/test_common.py::test_non_meta_estimators[QuantileTransformer-QuantileTransformer-check_dtype_object]\": {\"base\": 0.08836205676198006, \"head\": 0.10639665648341179}}, {\"sklearn/tests/test_common.py::test_non_meta_estimators[MiniBatchKMeans-MiniBatchKMeans-check_sample_weights_pandas_series]\": {\"base\": 0.23851877823472023, \"head\": 0.23859405890107155}, \"sklearn/tests/test_common.py::test_non_meta_estimators[NMF-NMF-check_fit_idempotent]\": {\"base\": 0.04533688351511955, \"head\": 0.1635533906519413}, \"sklearn/tests/test_common.py::test_non_meta_estimators[QuantileTransformer-QuantileTransformer-check_dtype_object]\": {\"base\": 0.011935919523239136, \"head\": 0.021936677396297455}}, {\"sklearn/tests/test_common.py::test_non_meta_estimators[MiniBatchKMeans-MiniBatchKMeans-check_sample_weights_pandas_series]\": {\"base\": 0.2374199517071247, \"head\": 0.23093818128108978}, \"sklearn/tests/test_common.py::test_non_meta_estimators[NMF-NMF-check_fit_idempotent]\": {\"base\": 0.02060982584953308, \"head\": 0.018514689058065414}, \"sklearn/tests/test_common.py::test_non_meta_estimators[QuantileTransformer-QuantileTransformer-check_dtype_object]\": {\"base\": 0.10358062759041786, \"head\": 0.01174589991569519}}, {\"sklearn/tests/test_common.py::test_non_meta_estimators[MiniBatchKMeans-MiniBatchKMeans-check_sample_weights_pandas_series]\": {\"base\": 0.23486147075891495, \"head\": 0.24145369604229927}, \"sklearn/tests/test_common.py::test_non_meta_estimators[NMF-NMF-check_fit_idempotent]\": {\"base\": 0.04634680226445198, \"head\": 0.02852664887905121}, \"sklearn/tests/test_common.py::test_non_meta_estimators[QuantileTransformer-QuantileTransformer-check_dtype_object]\": {\"base\": 0.09995194897055626, \"head\": 0.09491525962948799}}, {\"sklearn/tests/test_common.py::test_non_meta_estimators[MiniBatchKMeans-MiniBatchKMeans-check_sample_weights_pandas_series]\": {\"base\": 0.22927377000451088, \"head\": 0.23706751689314842}, \"sklearn/tests/test_common.py::test_non_meta_estimators[NMF-NMF-check_fit_idempotent]\": {\"base\": 0.016493376344442368, \"head\": 0.02643406391143799}, \"sklearn/tests/test_common.py::test_non_meta_estimators[QuantileTransformer-QuantileTransformer-check_dtype_object]\": {\"base\": 0.012203581631183624, \"head\": 0.011900149285793304}}, {\"sklearn/tests/test_common.py::test_non_meta_estimators[MiniBatchKMeans-MiniBatchKMeans-check_sample_weights_pandas_series]\": {\"base\": 0.2373766340315342, \"head\": 0.2450753152370453}, \"sklearn/tests/test_common.py::test_non_meta_estimators[NMF-NMF-check_fit_idempotent]\": {\"base\": 0.016750920563936234, \"head\": 0.02105560526251793}, \"sklearn/tests/test_common.py::test_non_meta_estimators[QuantileTransformer-QuantileTransformer-check_dtype_object]\": {\"base\": 0.014438923448324203, \"head\": 0.012165728956460953}}, {\"sklearn/tests/test_common.py::test_non_meta_estimators[MiniBatchKMeans-MiniBatchKMeans-check_sample_weights_pandas_series]\": {\"base\": 0.2436494193971157, \"head\": 0.23294232785701752}, \"sklearn/tests/test_common.py::test_non_meta_estimators[NMF-NMF-check_fit_idempotent]\": {\"base\": 0.017443247139453888, \"head\": 0.11632798612117767}, \"sklearn/tests/test_common.py::test_non_meta_estimators[QuantileTransformer-QuantileTransformer-check_dtype_object]\": {\"base\": 0.10437997803092003, \"head\": 0.08388230204582214}}, {\"sklearn/tests/test_common.py::test_non_meta_estimators[MiniBatchKMeans-MiniBatchKMeans-check_sample_weights_pandas_series]\": {\"base\": 0.2411254085600376, \"head\": 0.23228362947702408}, \"sklearn/tests/test_common.py::test_non_meta_estimators[NMF-NMF-check_fit_idempotent]\": {\"base\": 0.020122360438108444, \"head\": 0.021006718277931213}, \"sklearn/tests/test_common.py::test_non_meta_estimators[QuantileTransformer-QuantileTransformer-check_dtype_object]\": {\"base\": 0.01598406210541725, \"head\": 0.06659933924674988}}, {\"sklearn/tests/test_common.py::test_non_meta_estimators[MiniBatchKMeans-MiniBatchKMeans-check_sample_weights_pandas_series]\": {\"base\": 0.23707441985607147, \"head\": 0.22806233167648315}, \"sklearn/tests/test_common.py::test_non_meta_estimators[NMF-NMF-check_fit_idempotent]\": {\"base\": 0.017644643783569336, \"head\": 0.021781913936138153}, \"sklearn/tests/test_common.py::test_non_meta_estimators[QuantileTransformer-QuantileTransformer-check_dtype_object]\": {\"base\": 0.047145601361989975, \"head\": 0.07482529059052467}}, {\"sklearn/tests/test_common.py::test_non_meta_estimators[MiniBatchKMeans-MiniBatchKMeans-check_sample_weights_pandas_series]\": {\"base\": 0.24151015281677246, \"head\": 0.23143640533089638}, \"sklearn/tests/test_common.py::test_non_meta_estimators[NMF-NMF-check_fit_idempotent]\": {\"base\": 0.17821088060736656, \"head\": 0.019195709377527237}, \"sklearn/tests/test_common.py::test_non_meta_estimators[QuantileTransformer-QuantileTransformer-check_dtype_object]\": {\"base\": 0.012239467352628708, \"head\": 0.097438745200634}}, {\"sklearn/tests/test_common.py::test_non_meta_estimators[MiniBatchKMeans-MiniBatchKMeans-check_sample_weights_pandas_series]\": {\"base\": 0.26766176521778107, \"head\": 0.22839217260479927}, \"sklearn/tests/test_common.py::test_non_meta_estimators[NMF-NMF-check_fit_idempotent]\": {\"base\": 0.06153108924627304, \"head\": 0.025054577738046646}, \"sklearn/tests/test_common.py::test_non_meta_estimators[QuantileTransformer-QuantileTransformer-check_dtype_object]\": {\"base\": 0.10715948045253754, \"head\": 0.12453320994973183}}, {\"sklearn/tests/test_common.py::test_non_meta_estimators[MiniBatchKMeans-MiniBatchKMeans-check_sample_weights_pandas_series]\": {\"base\": 0.26654328033328056, \"head\": 0.22618751600384712}, \"sklearn/tests/test_common.py::test_non_meta_estimators[NMF-NMF-check_fit_idempotent]\": {\"base\": 0.013128060847520828, \"head\": 0.03957220911979675}, \"sklearn/tests/test_common.py::test_non_meta_estimators[QuantileTransformer-QuantileTransformer-check_dtype_object]\": {\"base\": 0.011828035116195679, \"head\": 0.1048538126051426}}, {\"sklearn/tests/test_common.py::test_non_meta_estimators[MiniBatchKMeans-MiniBatchKMeans-check_sample_weights_pandas_series]\": {\"base\": 0.2425316423177719, \"head\": 0.2368645817041397}, \"sklearn/tests/test_common.py::test_non_meta_estimators[NMF-NMF-check_fit_idempotent]\": {\"base\": 0.13619884848594666, \"head\": 0.03095852956175804}, \"sklearn/tests/test_common.py::test_non_meta_estimators[QuantileTransformer-QuantileTransformer-check_dtype_object]\": {\"base\": 0.022791672497987747, \"head\": 0.10433012247085571}}, {\"sklearn/tests/test_common.py::test_non_meta_estimators[MiniBatchKMeans-MiniBatchKMeans-check_sample_weights_pandas_series]\": {\"base\": 0.24418722838163376, \"head\": 0.23505256325006485}, \"sklearn/tests/test_common.py::test_non_meta_estimators[NMF-NMF-check_fit_idempotent]\": {\"base\": 0.031091079115867615, \"head\": 0.10165541619062424}, \"sklearn/tests/test_common.py::test_non_meta_estimators[QuantileTransformer-QuantileTransformer-check_dtype_object]\": {\"base\": 0.10157538205385208, \"head\": 0.08968248218297958}}, {\"sklearn/tests/test_common.py::test_non_meta_estimators[MiniBatchKMeans-MiniBatchKMeans-check_sample_weights_pandas_series]\": {\"base\": 0.23979071900248528, \"head\": 0.24272418767213821}, \"sklearn/tests/test_common.py::test_non_meta_estimators[NMF-NMF-check_fit_idempotent]\": {\"base\": 0.08406973630189896, \"head\": 0.021542951464653015}, \"sklearn/tests/test_common.py::test_non_meta_estimators[QuantileTransformer-QuantileTransformer-check_dtype_object]\": {\"base\": 0.08665557950735092, \"head\": 0.07927082851529121}}]",
  "efficiency_test": [
    "sklearn/tests/test_common.py::test_non_meta_estimators[MiniBatchKMeans-MiniBatchKMeans-check_sample_weights_pandas_series]",
    "sklearn/tests/test_common.py::test_non_meta_estimators[NMF-NMF-check_fit_idempotent]",
    "sklearn/tests/test_common.py::test_non_meta_estimators[QuantileTransformer-QuantileTransformer-check_dtype_object]"
  ],
  "patch_functions": "{\"sklearn/ensemble/weight_boosting.py\": [\"AdaBoostClassifier.fit\", \"BaseWeightBoosting.fit\", \"AdaBoostRegressor.fit\", \"AdaBoostClassifier.predict_log_proba\", \"AdaBoostClassifier.staged_predict_proba\", \"AdaBoostClassifier.decision_function\", \"AdaBoostRegressor._get_median_predict\", \"AdaBoostClassifier.predict\", \"AdaBoostClassifier.predict_proba\", \"BaseWeightBoosting._validate_X_predict\", \"BaseWeightBoosting.staged_score\", \"AdaBoostRegressor.predict\", \"AdaBoostClassifier.staged_predict\", \"AdaBoostClassifier.staged_decision_function\", \"AdaBoostClassifier._boost\", \"AdaBoostRegressor._boost\", \"AdaBoostRegressor.staged_predict\"]}",
  "problem_statement_oracle": "Please enhance the computational efficiency and execution speed across the entire repository. The optimization efforts may target one or more objective functions, including but not limited to:\n{'sklearn/ensemble/weight_boosting.py': ['AdaBoostClassifier.fit', 'BaseWeightBoosting.fit', 'AdaBoostRegressor.fit', 'AdaBoostClassifier.predict_log_proba', 'AdaBoostClassifier.staged_predict_proba', 'AdaBoostClassifier.decision_function', 'AdaBoostRegressor._get_median_predict', 'AdaBoostClassifier.predict', 'AdaBoostClassifier.predict_proba', 'BaseWeightBoosting._validate_X_predict', 'BaseWeightBoosting.staged_score', 'AdaBoostRegressor.predict', 'AdaBoostClassifier.staged_predict', 'AdaBoostClassifier.staged_decision_function', 'AdaBoostClassifier._boost', 'AdaBoostRegressor._boost', 'AdaBoostRegressor.staged_predict']}\nThe following conditions apply:\n1. Acceleration of at least one objective function is sufficient for success, as performance evaluations will be conducted collectively on all targeted functions.\n2. Optimization may be achieved either directly through modifications to the objective functions or indirectly by improving computationally intensive subroutines upon which they depend.\n3. Optimization efforts should prioritize maximal efficiency gains where feasible.\n4. All existing unit tests must remain unaltered to preserve functional correctness.",
  "test_functions": "{\"sklearn/utils/estimator_checks.py\": [\"_yield_all_checks\", \"set_checking_parameters\", \"check_fit_idempotent\"], \"sklearn/utils/deprecation.py\": [\"wrapped\"], \"sklearn/base.py\": [\"AdaBoostClassifier.get_params\"]}",
  "problem_statement_realistic": "Please enhance the computational efficiency and execution speed across the entire repository. The optimization efforts may target one or more objective functions, including but not limited to:\ndefaultdict(<class 'list'>, {'sklearn/utils/estimator_checks.py': ['_yield_all_checks', 'set_checking_parameters', 'check_fit_idempotent'], 'sklearn/utils/deprecation.py': ['wrapped'], 'sklearn/base.py': ['AdaBoostClassifier.get_params']})\nThe following conditions apply:\n1. Acceleration of at least one objective function is sufficient for success, as performance evaluations will be conducted collectively on all targeted functions.\n2. Optimization may be achieved either directly through modifications to the objective functions or indirectly by improving computationally intensive subroutines upon which they depend.\n3. Optimization efforts should prioritize maximal efficiency gains where feasible.\n4. All existing unit tests must remain unaltered to preserve functional correctness.",
  "human_performance": 0.004000000000000001
}