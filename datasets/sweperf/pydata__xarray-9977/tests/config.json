{
  "repo": "pydata/xarray",
  "instance_id": "pydata__xarray-9977",
  "patch": "diff --git a/doc/whats-new.rst b/doc/whats-new.rst\nindex 9b40a323f39..8a9ba2d2ea3 100644\n--- a/doc/whats-new.rst\n+++ b/doc/whats-new.rst\n@@ -50,7 +50,7 @@ eventually be deprecated.\n \n New Features\n ~~~~~~~~~~~~\n-- Relax nanosecond datetime restriction in CF time decoding (:issue:`7493`, :pull:`9618`).\n+- Relax nanosecond datetime restriction in CF time decoding (:issue:`7493`, :pull:`9618`, :pull:`9977`).\n   By `Kai M\u00fchlbauer <https://github.com/kmuehlbauer>`_ and `Spencer Clark <https://github.com/spencerkclark>`_.\n - Improve the error message raised when no key is matching the available variables in a dataset.  (:pull:`9943`)\n   By `Jimmy Westling <https://github.com/illviljan>`_.\ndiff --git a/xarray/core/duck_array_ops.py b/xarray/core/duck_array_ops.py\nindex 7e7333fd8ea..c3f1598050a 100644\n--- a/xarray/core/duck_array_ops.py\n+++ b/xarray/core/duck_array_ops.py\n@@ -662,16 +662,10 @@ def _to_pytimedelta(array, unit=\"us\"):\n \n \n def np_timedelta64_to_float(array, datetime_unit):\n-    \"\"\"Convert numpy.timedelta64 to float.\n-\n-    Notes\n-    -----\n-    The array is first converted to microseconds, which is less likely to\n-    cause overflow errors.\n-    \"\"\"\n-    array = array.astype(\"timedelta64[ns]\").astype(np.float64)\n-    conversion_factor = np.timedelta64(1, \"ns\") / np.timedelta64(1, datetime_unit)\n-    return conversion_factor * array\n+    \"\"\"Convert numpy.timedelta64 to float, possibly at a loss of resolution.\"\"\"\n+    unit, _ = np.datetime_data(array.dtype)\n+    conversion_factor = np.timedelta64(1, unit) / np.timedelta64(1, datetime_unit)\n+    return conversion_factor * array.astype(np.float64)\n \n \n def pd_timedelta_to_float(value, datetime_unit):\n@@ -715,12 +709,15 @@ def mean(array, axis=None, skipna=None, **kwargs):\n     if dtypes.is_datetime_like(array.dtype):\n         offset = _datetime_nanmin(array)\n \n-        # xarray always uses np.datetime64[ns] for np.datetime64 data\n-        dtype = \"timedelta64[ns]\"\n+        # From version 2025.01.2 xarray uses np.datetime64[unit], where unit\n+        # is one of \"s\", \"ms\", \"us\", \"ns\".\n+        # To not have to worry about the resolution, we just convert the output\n+        # to \"timedelta64\" (without unit) and let the dtype of offset take precedence.\n+        # This is fully backwards compatible with datetime64[ns].\n         return (\n             _mean(\n                 datetime_to_numeric(array, offset), axis=axis, skipna=skipna, **kwargs\n-            ).astype(dtype)\n+            ).astype(\"timedelta64\")\n             + offset\n         )\n     elif _contains_cftime_datetimes(array):\n",
  "test_patch": "diff --git a/xarray/tests/test_duck_array_ops.py b/xarray/tests/test_duck_array_ops.py\nindex e1306964757..9b8b50d99a1 100644\n--- a/xarray/tests/test_duck_array_ops.py\n+++ b/xarray/tests/test_duck_array_ops.py\n@@ -9,6 +9,7 @@\n from numpy import array, nan\n \n from xarray import DataArray, Dataset, cftime_range, concat\n+from xarray.coding.times import _NS_PER_TIME_DELTA\n from xarray.core import dtypes, duck_array_ops\n from xarray.core.duck_array_ops import (\n     array_notnull_equiv,\n@@ -28,6 +29,7 @@\n     where,\n )\n from xarray.core.extension_array import PandasExtensionArray\n+from xarray.core.types import NPDatetimeUnitOptions, PDDatetimeUnitOptions\n from xarray.namedarray.pycompat import array_type\n from xarray.testing import assert_allclose, assert_equal, assert_identical\n from xarray.tests import (\n@@ -411,10 +413,11 @@ def assert_dask_array(da, dask):\n @arm_xfail\n @pytest.mark.filterwarnings(\"ignore:All-NaN .* encountered:RuntimeWarning\")\n @pytest.mark.parametrize(\"dask\", [False, True] if has_dask else [False])\n-def test_datetime_mean(dask: bool) -> None:\n+def test_datetime_mean(dask: bool, time_unit: PDDatetimeUnitOptions) -> None:\n     # Note: only testing numpy, as dask is broken upstream\n+    dtype = f\"M8[{time_unit}]\"\n     da = DataArray(\n-        np.array([\"2010-01-01\", \"NaT\", \"2010-01-03\", \"NaT\", \"NaT\"], dtype=\"M8[ns]\"),\n+        np.array([\"2010-01-01\", \"NaT\", \"2010-01-03\", \"NaT\", \"NaT\"], dtype=dtype),\n         dims=[\"time\"],\n     )\n     if dask:\n@@ -846,11 +849,11 @@ def test_multiple_dims(dtype, dask, skipna, func):\n \n \n @pytest.mark.parametrize(\"dask\", [True, False])\n-def test_datetime_to_numeric_datetime64(dask):\n+def test_datetime_to_numeric_datetime64(dask, time_unit: PDDatetimeUnitOptions):\n     if dask and not has_dask:\n         pytest.skip(\"requires dask\")\n \n-    times = pd.date_range(\"2000\", periods=5, freq=\"7D\").values\n+    times = pd.date_range(\"2000\", periods=5, freq=\"7D\").as_unit(time_unit).values\n     if dask:\n         import dask.array\n \n@@ -874,8 +877,8 @@ def test_datetime_to_numeric_datetime64(dask):\n         result = duck_array_ops.datetime_to_numeric(\n             times, datetime_unit=\"h\", dtype=dtype\n         )\n-    expected = 24 * np.arange(0, 35, 7).astype(dtype)\n-    np.testing.assert_array_equal(result, expected)\n+    expected2 = 24 * np.arange(0, 35, 7).astype(dtype)\n+    np.testing.assert_array_equal(result, expected2)\n \n \n @requires_cftime\n@@ -923,15 +926,18 @@ def test_datetime_to_numeric_cftime(dask):\n \n \n @requires_cftime\n-def test_datetime_to_numeric_potential_overflow():\n+def test_datetime_to_numeric_potential_overflow(time_unit: PDDatetimeUnitOptions):\n     import cftime\n \n-    times = pd.date_range(\"2000\", periods=5, freq=\"7D\").values.astype(\"datetime64[us]\")\n+    if time_unit == \"ns\":\n+        pytest.skip(\"out-of-bounds datetime64 overflow\")\n+    dtype = f\"M8[{time_unit}]\"\n+    times = pd.date_range(\"2000\", periods=5, freq=\"7D\").values.astype(dtype)\n     cftimes = cftime_range(\n         \"2000\", periods=5, freq=\"7D\", calendar=\"proleptic_gregorian\"\n     ).values\n \n-    offset = np.datetime64(\"0001-01-01\")\n+    offset = np.datetime64(\"0001-01-01\", time_unit)\n     cfoffset = cftime.DatetimeProlepticGregorian(1, 1, 1)\n \n     result = duck_array_ops.datetime_to_numeric(\n@@ -957,24 +963,33 @@ def test_py_timedelta_to_float():\n     assert py_timedelta_to_float(dt.timedelta(days=1e6), \"D\") == 1e6\n \n \n-@pytest.mark.parametrize(\n-    \"td, expected\",\n-    ([np.timedelta64(1, \"D\"), 86400 * 1e9], [np.timedelta64(1, \"ns\"), 1.0]),\n-)\n-def test_np_timedelta64_to_float(td, expected):\n-    out = np_timedelta64_to_float(td, datetime_unit=\"ns\")\n+@pytest.mark.parametrize(\"np_dt_unit\", [\"D\", \"h\", \"m\", \"s\", \"ms\", \"us\", \"ns\"])\n+def test_np_timedelta64_to_float(\n+    np_dt_unit: NPDatetimeUnitOptions, time_unit: PDDatetimeUnitOptions\n+):\n+    # tests any combination of source np.timedelta64 (NPDatetimeUnitOptions) with\n+    # np_timedelta_to_float with dedicated target unit (PDDatetimeUnitOptions)\n+    td = np.timedelta64(1, np_dt_unit)\n+    expected = _NS_PER_TIME_DELTA[np_dt_unit] / _NS_PER_TIME_DELTA[time_unit]\n+\n+    out = np_timedelta64_to_float(td, datetime_unit=time_unit)\n     np.testing.assert_allclose(out, expected)\n     assert isinstance(out, float)\n \n-    out = np_timedelta64_to_float(np.atleast_1d(td), datetime_unit=\"ns\")\n+    out = np_timedelta64_to_float(np.atleast_1d(td), datetime_unit=time_unit)\n     np.testing.assert_allclose(out, expected)\n \n \n-@pytest.mark.parametrize(\n-    \"td, expected\", ([pd.Timedelta(1, \"D\"), 86400 * 1e9], [pd.Timedelta(1, \"ns\"), 1.0])\n-)\n-def test_pd_timedelta_to_float(td, expected):\n-    out = pd_timedelta_to_float(td, datetime_unit=\"ns\")\n+@pytest.mark.parametrize(\"np_dt_unit\", [\"D\", \"h\", \"m\", \"s\", \"ms\", \"us\", \"ns\"])\n+def test_pd_timedelta_to_float(\n+    np_dt_unit: NPDatetimeUnitOptions, time_unit: PDDatetimeUnitOptions\n+):\n+    # tests any combination of source pd.Timedelta (NPDatetimeUnitOptions) with\n+    # np_timedelta_to_float with dedicated target unit (PDDatetimeUnitOptions)\n+    td = pd.Timedelta(1, np_dt_unit)\n+    expected = _NS_PER_TIME_DELTA[np_dt_unit] / _NS_PER_TIME_DELTA[time_unit]\n+\n+    out = pd_timedelta_to_float(td, datetime_unit=time_unit)\n     np.testing.assert_allclose(out, expected)\n     assert isinstance(out, float)\n \n@@ -982,10 +997,11 @@ def test_pd_timedelta_to_float(td, expected):\n @pytest.mark.parametrize(\n     \"td\", [dt.timedelta(days=1), np.timedelta64(1, \"D\"), pd.Timedelta(1, \"D\"), \"1 day\"]\n )\n-def test_timedelta_to_numeric(td):\n+def test_timedelta_to_numeric(td, time_unit: PDDatetimeUnitOptions):\n     # Scalar input\n-    out = timedelta_to_numeric(td, \"ns\")\n-    np.testing.assert_allclose(out, 86400 * 1e9)\n+    out = timedelta_to_numeric(td, time_unit)\n+    expected = _NS_PER_TIME_DELTA[\"D\"] / _NS_PER_TIME_DELTA[time_unit]\n+    np.testing.assert_allclose(out, expected)\n     assert isinstance(out, float)\n \n \n",
  "base_commit": "d62b276751b0f8b412e2b22aeb0f309f6bab7a27",
  "head_commit": "892fc07a6f500124a7d84f78004b460790afc710",
  "created_at": 1737620558000,
  "version": "2024.05",
  "duration_changes": "[{\"xarray/tests/test_rolling.py::TestDataArrayRolling::test_rolling_reduce[numbagg-numpy-sum-1-2-False-1]\": {\"base\": 1.7046984061598778, \"head\": 1.8533587455749512}}, {\"xarray/tests/test_rolling.py::TestDataArrayRolling::test_rolling_reduce[numbagg-numpy-sum-1-2-False-1]\": {\"base\": 1.6226349920034409, \"head\": 1.7452248968183994}}, {\"xarray/tests/test_rolling.py::TestDataArrayRolling::test_rolling_reduce[numbagg-numpy-sum-1-2-False-1]\": {\"base\": 1.812995307147503, \"head\": 1.6397889219224453}}, {\"xarray/tests/test_rolling.py::TestDataArrayRolling::test_rolling_reduce[numbagg-numpy-sum-1-2-False-1]\": {\"base\": 1.7119727544486523, \"head\": 1.6259532868862152}}, {\"xarray/tests/test_rolling.py::TestDataArrayRolling::test_rolling_reduce[numbagg-numpy-sum-1-2-False-1]\": {\"base\": 1.7336510084569454, \"head\": 1.613757185637951}}, {\"xarray/tests/test_rolling.py::TestDataArrayRolling::test_rolling_reduce[numbagg-numpy-sum-1-2-False-1]\": {\"base\": 1.752272754907608, \"head\": 1.6767989173531532}}, {\"xarray/tests/test_rolling.py::TestDataArrayRolling::test_rolling_reduce[numbagg-numpy-sum-1-2-False-1]\": {\"base\": 1.7324811480939388, \"head\": 1.7548604607582092}}, {\"xarray/tests/test_rolling.py::TestDataArrayRolling::test_rolling_reduce[numbagg-numpy-sum-1-2-False-1]\": {\"base\": 1.7030625492334366, \"head\": 1.8194482661783695}}, {\"xarray/tests/test_rolling.py::TestDataArrayRolling::test_rolling_reduce[numbagg-numpy-sum-1-2-False-1]\": {\"base\": 1.7161859534680843, \"head\": 1.671976737678051}}, {\"xarray/tests/test_rolling.py::TestDataArrayRolling::test_rolling_reduce[numbagg-numpy-sum-1-2-False-1]\": {\"base\": 1.6835092194378376, \"head\": 1.8701528161764145}}, {\"xarray/tests/test_rolling.py::TestDataArrayRolling::test_rolling_reduce[numbagg-numpy-sum-1-2-False-1]\": {\"base\": 1.7728401310741901, \"head\": 1.6917267926037312}}, {\"xarray/tests/test_rolling.py::TestDataArrayRolling::test_rolling_reduce[numbagg-numpy-sum-1-2-False-1]\": {\"base\": 1.7470750287175179, \"head\": 1.6750119775533676}}, {\"xarray/tests/test_rolling.py::TestDataArrayRolling::test_rolling_reduce[numbagg-numpy-sum-1-2-False-1]\": {\"base\": 1.7922820076346397, \"head\": 1.815646268427372}}, {\"xarray/tests/test_rolling.py::TestDataArrayRolling::test_rolling_reduce[numbagg-numpy-sum-1-2-False-1]\": {\"base\": 1.8289928995072842, \"head\": 1.839482143521309}}, {\"xarray/tests/test_rolling.py::TestDataArrayRolling::test_rolling_reduce[numbagg-numpy-sum-1-2-False-1]\": {\"base\": 1.7978964373469353, \"head\": 1.6802800297737122}}, {\"xarray/tests/test_rolling.py::TestDataArrayRolling::test_rolling_reduce[numbagg-numpy-sum-1-2-False-1]\": {\"base\": 1.7489696331322193, \"head\": 1.686872836202383}}, {\"xarray/tests/test_rolling.py::TestDataArrayRolling::test_rolling_reduce[numbagg-numpy-sum-1-2-False-1]\": {\"base\": 1.6965582557022572, \"head\": 1.7285321317613125}}, {\"xarray/tests/test_rolling.py::TestDataArrayRolling::test_rolling_reduce[numbagg-numpy-sum-1-2-False-1]\": {\"base\": 1.8279180824756622, \"head\": 1.6555584333837032}}, {\"xarray/tests/test_rolling.py::TestDataArrayRolling::test_rolling_reduce[numbagg-numpy-sum-1-2-False-1]\": {\"base\": 1.7265439964830875, \"head\": 1.7218557186424732}}, {\"xarray/tests/test_rolling.py::TestDataArrayRolling::test_rolling_reduce[numbagg-numpy-sum-1-2-False-1]\": {\"base\": 1.7689036652445793, \"head\": 1.7474991120398045}}]",
  "efficiency_test": [
    "xarray/tests/test_rolling.py::TestDataArrayRolling::test_rolling_reduce[numbagg-numpy-sum-1-2-False-1]"
  ],
  "patch_functions": "{\"xarray/core/duck_array_ops.py\": [\"np_timedelta64_to_float\", \"mean\"]}",
  "problem_statement_oracle": "Please enhance the computational efficiency and execution speed across the entire repository. The optimization efforts may target one or more objective functions, including but not limited to:\n{'xarray/core/duck_array_ops.py': ['np_timedelta64_to_float', 'mean']}\nThe following conditions apply:\n1. Acceleration of at least one objective function is sufficient for success, as performance evaluations will be conducted collectively on all targeted functions.\n2. Optimization may be achieved either directly through modifications to the objective functions or indirectly by improving computationally intensive subroutines upon which they depend.\n3. Optimization efforts should prioritize maximal efficiency gains where feasible.\n4. All existing unit tests must remain unaltered to preserve functional correctness.",
  "test_functions": "{\"xarray/core/variable.py\": [\"Variable.reduce\"], \"xarray/namedarray/core.py\": [\"Variable.reduce\"], \"xarray/core/common.py\": [\"DataArray.isnull\", \"DataArray.sizes\"], \"xarray/core/dataarray.py\": [\"DataArray.rolling\", \"DataArray.reduce\"], \"xarray/core/_aggregations.py\": [\"DataArray.sum\"], \"xarray/core/rolling.py\": [\"DataArrayRolling.method\", \"DataArrayRolling.reduce\"]}",
  "problem_statement_realistic": "Please enhance the computational efficiency and execution speed across the entire repository. The optimization efforts may target one or more objective functions, including but not limited to:\ndefaultdict(<class 'list'>, {'xarray/core/variable.py': ['Variable.reduce'], 'xarray/namedarray/core.py': ['Variable.reduce'], 'xarray/core/common.py': ['DataArray.isnull', 'DataArray.sizes'], 'xarray/core/dataarray.py': ['DataArray.rolling', 'DataArray.reduce'], 'xarray/core/_aggregations.py': ['DataArray.sum'], 'xarray/core/rolling.py': ['DataArrayRolling.method', 'DataArrayRolling.reduce']})\nThe following conditions apply:\n1. Acceleration of at least one objective function is sufficient for success, as performance evaluations will be conducted collectively on all targeted functions.\n2. Optimization may be achieved either directly through modifications to the objective functions or indirectly by improving computationally intensive subroutines upon which they depend.\n3. Optimization efforts should prioritize maximal efficiency gains where feasible.\n4. All existing unit tests must remain unaltered to preserve functional correctness.",
  "human_performance": 0.006
}