{
  "repo": "scikit-learn/scikit-learn",
  "instance_id": "scikit-learn__scikit-learn-12446",
  "patch": "diff --git a/sklearn/preprocessing/data.py b/sklearn/preprocessing/data.py\nindex f2a7dbaa16dc0..02fc096a88e60 100644\n--- a/sklearn/preprocessing/data.py\n+++ b/sklearn/preprocessing/data.py\n@@ -480,6 +480,14 @@ def minmax_scale(X, feature_range=(0, 1), axis=0, copy=True):\n class StandardScaler(BaseEstimator, TransformerMixin):\n     \"\"\"Standardize features by removing the mean and scaling to unit variance\n \n+    The standard score of a sample `x` is calculated as:\n+\n+        z = (x - u) / s\n+\n+    where `u` is the mean of the training samples or zero if `with_mean=False`,\n+    and `s` is the standard deviation of the training samples or one if\n+    `with_std=False`.\n+\n     Centering and scaling happen independently on each feature by computing\n     the relevant statistics on the samples in the training set. Mean and\n     standard deviation are then stored to be used on later data using the\n@@ -525,8 +533,8 @@ class StandardScaler(BaseEstimator, TransformerMixin):\n     Attributes\n     ----------\n     scale_ : ndarray or None, shape (n_features,)\n-        Per feature relative scaling of the data. Equal to ``None`` when\n-        ``with_std=False``.\n+        Per feature relative scaling of the data. This is calculated using\n+        `np.sqrt(var_)`. Equal to ``None`` when ``with_std=False``.\n \n         .. versionadded:: 0.17\n            *scale_*\n",
  "test_patch": "",
  "base_commit": "e50fc2af940203226a9a83836a14d679c94a7486",
  "head_commit": "5c1e357dbd315fe182fc9f9dc690be2ea179f463",
  "created_at": 1540322946000,
  "version": "0.21",
  "duration_changes": "[{\"sklearn/covariance/tests/test_robust_covariance.py::test_mcd_increasing_det_warning\": {\"base\": 0.040153395384550095, \"head\": 0.17078200727701187}, \"sklearn/decomposition/tests/test_online_lda.py::test_lda_dense_input\": {\"base\": 0.09376529604196548, \"head\": 0.01179259642958641}, \"sklearn/preprocessing/tests/test_data.py::test_power_transformer_nans[box-cox]\": {\"base\": 0.0049673691391944885, \"head\": 0.004801146686077118}}, {\"sklearn/covariance/tests/test_robust_covariance.py::test_mcd_increasing_det_warning\": {\"base\": 0.029046084731817245, \"head\": 0.13638587296009064}, \"sklearn/decomposition/tests/test_online_lda.py::test_lda_dense_input\": {\"base\": 0.02263297140598297, \"head\": 0.013860967010259628}, \"sklearn/preprocessing/tests/test_data.py::test_power_transformer_nans[box-cox]\": {\"base\": 0.005852404981851578, \"head\": 0.004676613956689835}}, {\"sklearn/covariance/tests/test_robust_covariance.py::test_mcd_increasing_det_warning\": {\"base\": 0.13588830456137657, \"head\": 0.15106486529111862}, \"sklearn/decomposition/tests/test_online_lda.py::test_lda_dense_input\": {\"base\": 0.011638838797807693, \"head\": 0.011541023850440979}, \"sklearn/preprocessing/tests/test_data.py::test_power_transformer_nans[box-cox]\": {\"base\": 0.004769362509250641, \"head\": 0.004653759300708771}}, {\"sklearn/covariance/tests/test_robust_covariance.py::test_mcd_increasing_det_warning\": {\"base\": 0.043692268431186676, \"head\": 0.16418664157390594}, \"sklearn/decomposition/tests/test_online_lda.py::test_lda_dense_input\": {\"base\": 0.08449874445796013, \"head\": 0.01657094433903694}, \"sklearn/preprocessing/tests/test_data.py::test_power_transformer_nans[box-cox]\": {\"base\": 0.005670282989740372, \"head\": 0.005271892994642258}}, {\"sklearn/covariance/tests/test_robust_covariance.py::test_mcd_increasing_det_warning\": {\"base\": 0.14606228098273277, \"head\": 0.034701745957136154}, \"sklearn/decomposition/tests/test_online_lda.py::test_lda_dense_input\": {\"base\": 0.011491458863019943, \"head\": 0.08742710947990417}, \"sklearn/preprocessing/tests/test_data.py::test_power_transformer_nans[box-cox]\": {\"base\": 0.004685100167989731, \"head\": 0.0064879730343818665}}, {\"sklearn/covariance/tests/test_robust_covariance.py::test_mcd_increasing_det_warning\": {\"base\": 0.19483792036771774, \"head\": 0.15760917216539383}, \"sklearn/decomposition/tests/test_online_lda.py::test_lda_dense_input\": {\"base\": 0.016735181212425232, \"head\": 0.011777300387620926}, \"sklearn/preprocessing/tests/test_data.py::test_power_transformer_nans[box-cox]\": {\"base\": 0.006265949457883835, \"head\": 0.004699960350990295}}, {\"sklearn/covariance/tests/test_robust_covariance.py::test_mcd_increasing_det_warning\": {\"base\": 0.1459413804113865, \"head\": 0.06367379799485207}, \"sklearn/decomposition/tests/test_online_lda.py::test_lda_dense_input\": {\"base\": 0.05545254796743393, \"head\": 0.09646066278219223}, \"sklearn/preprocessing/tests/test_data.py::test_power_transformer_nans[box-cox]\": {\"base\": 0.005774188786745071, \"head\": 0.010697036981582642}}, {\"sklearn/covariance/tests/test_robust_covariance.py::test_mcd_increasing_det_warning\": {\"base\": 0.039126913994550705, \"head\": 0.14171681180596352}, \"sklearn/decomposition/tests/test_online_lda.py::test_lda_dense_input\": {\"base\": 0.1236247569322586, \"head\": 0.01187097653746605}, \"sklearn/preprocessing/tests/test_data.py::test_power_transformer_nans[box-cox]\": {\"base\": 0.005396712571382523, \"head\": 0.004780281335115433}}, {\"sklearn/covariance/tests/test_robust_covariance.py::test_mcd_increasing_det_warning\": {\"base\": 0.15380903333425522, \"head\": 0.11247853562235832}, \"sklearn/decomposition/tests/test_online_lda.py::test_lda_dense_input\": {\"base\": 0.011628631502389908, \"head\": 0.016654960811138153}, \"sklearn/preprocessing/tests/test_data.py::test_power_transformer_nans[box-cox]\": {\"base\": 0.004728455096483231, \"head\": 0.006496436893939972}}, {\"sklearn/covariance/tests/test_robust_covariance.py::test_mcd_increasing_det_warning\": {\"base\": 0.1190420500934124, \"head\": 0.16306762769818306}, \"sklearn/decomposition/tests/test_online_lda.py::test_lda_dense_input\": {\"base\": 0.08269288763403893, \"head\": 0.011520534753799438}, \"sklearn/preprocessing/tests/test_data.py::test_power_transformer_nans[box-cox]\": {\"base\": 0.006772536784410477, \"head\": 0.004642050713300705}}, {\"sklearn/covariance/tests/test_robust_covariance.py::test_mcd_increasing_det_warning\": {\"base\": 0.033342648297548294, \"head\": 0.17034664750099182}, \"sklearn/decomposition/tests/test_online_lda.py::test_lda_dense_input\": {\"base\": 0.016590293496847153, \"head\": 0.012078136205673218}, \"sklearn/preprocessing/tests/test_data.py::test_power_transformer_nans[box-cox]\": {\"base\": 0.0062369778752326965, \"head\": 0.004684899002313614}}, {\"sklearn/covariance/tests/test_robust_covariance.py::test_mcd_increasing_det_warning\": {\"base\": 0.07432588934898376, \"head\": 0.1849900782108307}, \"sklearn/decomposition/tests/test_online_lda.py::test_lda_dense_input\": {\"base\": 0.09788971021771431, \"head\": 0.01435791328549385}, \"sklearn/preprocessing/tests/test_data.py::test_power_transformer_nans[box-cox]\": {\"base\": 0.008071858435869217, \"head\": 0.004880465567111969}}, {\"sklearn/covariance/tests/test_robust_covariance.py::test_mcd_increasing_det_warning\": {\"base\": 0.1619349867105484, \"head\": 0.13951260596513748}, \"sklearn/decomposition/tests/test_online_lda.py::test_lda_dense_input\": {\"base\": 0.01586752012372017, \"head\": 0.01166585087776184}, \"sklearn/preprocessing/tests/test_data.py::test_power_transformer_nans[box-cox]\": {\"base\": 0.004718661308288574, \"head\": 0.004753731191158295}}, {\"sklearn/covariance/tests/test_robust_covariance.py::test_mcd_increasing_det_warning\": {\"base\": 0.19329224154353142, \"head\": 0.029449328780174255}, \"sklearn/decomposition/tests/test_online_lda.py::test_lda_dense_input\": {\"base\": 0.015686091035604477, \"head\": 0.17915180698037148}, \"sklearn/preprocessing/tests/test_data.py::test_power_transformer_nans[box-cox]\": {\"base\": 0.004717763513326645, \"head\": 0.004904542118310928}}, {\"sklearn/covariance/tests/test_robust_covariance.py::test_mcd_increasing_det_warning\": {\"base\": 0.1256108619272709, \"head\": 0.030604474246501923}, \"sklearn/decomposition/tests/test_online_lda.py::test_lda_dense_input\": {\"base\": 0.01850723847746849, \"head\": 0.012825824320316315}, \"sklearn/preprocessing/tests/test_data.py::test_power_transformer_nans[box-cox]\": {\"base\": 0.0046815574169158936, \"head\": 0.005164232105016708}}, {\"sklearn/covariance/tests/test_robust_covariance.py::test_mcd_increasing_det_warning\": {\"base\": 0.1523727923631668, \"head\": 0.14192535355687141}, \"sklearn/decomposition/tests/test_online_lda.py::test_lda_dense_input\": {\"base\": 0.014406051486730576, \"head\": 0.011409841477870941}, \"sklearn/preprocessing/tests/test_data.py::test_power_transformer_nans[box-cox]\": {\"base\": 0.0047330595552921295, \"head\": 0.004647679626941681}}, {\"sklearn/covariance/tests/test_robust_covariance.py::test_mcd_increasing_det_warning\": {\"base\": 0.05028050020337105, \"head\": 0.15368405729532242}, \"sklearn/decomposition/tests/test_online_lda.py::test_lda_dense_input\": {\"base\": 0.01866253837943077, \"head\": 0.014382511377334595}, \"sklearn/preprocessing/tests/test_data.py::test_power_transformer_nans[box-cox]\": {\"base\": 0.007133856415748596, \"head\": 0.0048195235431194305}}, {\"sklearn/covariance/tests/test_robust_covariance.py::test_mcd_increasing_det_warning\": {\"base\": 0.14264921471476555, \"head\": 0.0885867141187191}, \"sklearn/decomposition/tests/test_online_lda.py::test_lda_dense_input\": {\"base\": 0.012111317366361618, \"head\": 0.01908261701464653}, \"sklearn/preprocessing/tests/test_data.py::test_power_transformer_nans[box-cox]\": {\"base\": 0.004808105528354645, \"head\": 0.07158384844660759}}, {\"sklearn/covariance/tests/test_robust_covariance.py::test_mcd_increasing_det_warning\": {\"base\": 0.14050541818141937, \"head\": 0.048378195613622665}, \"sklearn/decomposition/tests/test_online_lda.py::test_lda_dense_input\": {\"base\": 0.018269982188940048, \"head\": 0.05723964050412178}, \"sklearn/preprocessing/tests/test_data.py::test_power_transformer_nans[box-cox]\": {\"base\": 0.00471150130033493, \"head\": 0.00653037428855896}}, {\"sklearn/covariance/tests/test_robust_covariance.py::test_mcd_increasing_det_warning\": {\"base\": 0.13872209191322327, \"head\": 0.036740995943546295}, \"sklearn/decomposition/tests/test_online_lda.py::test_lda_dense_input\": {\"base\": 0.020460981875658035, \"head\": 0.07700584456324577}, \"sklearn/preprocessing/tests/test_data.py::test_power_transformer_nans[box-cox]\": {\"base\": 0.007383175194263458, \"head\": 0.005001123994588852}}]",
  "efficiency_test": [
    "sklearn/covariance/tests/test_robust_covariance.py::test_mcd_increasing_det_warning",
    "sklearn/decomposition/tests/test_online_lda.py::test_lda_dense_input",
    "sklearn/preprocessing/tests/test_data.py::test_power_transformer_nans[box-cox]"
  ],
  "patch_functions": "{}",
  "problem_statement_oracle": "Please enhance the computational efficiency and execution speed across the entire repository. The optimization efforts may target one or more objective functions, including but not limited to:\n{}\nThe following conditions apply:\n1. Acceleration of at least one objective function is sufficient for success, as performance evaluations will be conducted collectively on all targeted functions.\n2. Optimization may be achieved either directly through modifications to the objective functions or indirectly by improving computationally intensive subroutines upon which they depend.\n3. Optimization efforts should prioritize maximal efficiency gains where feasible.\n4. All existing unit tests must remain unaltered to preserve functional correctness.",
  "test_functions": "{\"sklearn/preprocessing/data.py\": [\"StandardScaler.transform\", \"PowerTransformer.transform\", \"PowerTransformer.fit\", \"PowerTransformer\", \"StandardScaler.fit\"], \"sklearn/utils/__init__.py\": [\"shuffle\"], \"sklearn/decomposition/online_lda.py\": [\"LatentDirichletAllocation\", \"LatentDirichletAllocation.fit\"], \"sklearn/datasets/base.py\": [\"load_iris\"], \"sklearn/covariance/robust_covariance.py\": [\"MinCovDet\"]}",
  "problem_statement_realistic": "Please enhance the computational efficiency and execution speed across the entire repository. The optimization efforts may target one or more objective functions, including but not limited to:\ndefaultdict(<class 'list'>, {'sklearn/preprocessing/data.py': ['StandardScaler.transform', 'PowerTransformer.transform', 'PowerTransformer.fit', 'PowerTransformer', 'StandardScaler.fit'], 'sklearn/utils/__init__.py': ['shuffle'], 'sklearn/decomposition/online_lda.py': ['LatentDirichletAllocation', 'LatentDirichletAllocation.fit'], 'sklearn/datasets/base.py': ['load_iris'], 'sklearn/covariance/robust_covariance.py': ['MinCovDet']})\nThe following conditions apply:\n1. Acceleration of at least one objective function is sufficient for success, as performance evaluations will be conducted collectively on all targeted functions.\n2. Optimization may be achieved either directly through modifications to the objective functions or indirectly by improving computationally intensive subroutines upon which they depend.\n3. Optimization efforts should prioritize maximal efficiency gains where feasible.\n4. All existing unit tests must remain unaltered to preserve functional correctness.",
  "human_performance": 0.06366666666666672
}